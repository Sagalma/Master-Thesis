{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import of libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "from struct import*\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "from numpy import nan\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading file, shaping 3D matrix and plot of CT/PET."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to read info_text file.\n",
    "def read_info_txt(file):\n",
    "    with open(file, mode='r') as file: #Reading PET or CT file\n",
    "        fileContent = file.read()\n",
    "        SplitFile = fileContent.split()\n",
    "        array_info_txt = []\n",
    "        for i in range(len(SplitFile)):\n",
    "            array_info_txt.append(float(SplitFile[i]))\n",
    "    return array_info_txt\n",
    "\n",
    "\n",
    "# Function to read data, and create a 3D matrix\n",
    "def read_CT_PET(PatientNumber,Image='CT'):\n",
    "    # Add up strings to form path to extract image\n",
    "    Part_readfile = '/volumes/LaCie/Sagal/HN_ANON_09102017/Patient_Folders/'\n",
    "    strek ='/'\n",
    "    if Image == 'CT':\n",
    "        Imaging = 'CT'\n",
    "    else:\n",
    "        Imaging = 'PT'\n",
    "    file = Part_readfile+PatientNumber+strek+Imaging\n",
    "    with open(file, mode='rb') as file: #Reading PET or CT file\n",
    "        fileContent = file.read()\n",
    "    Unpack = unpack(\"H\" * ((len(fileContent)//2)), fileContent) # Converts data stored as bytes to an array.\n",
    "    # Read InfoTextFile\n",
    "    remaining = '_info.txt'\n",
    "    InfoTextFile = Part_readfile + PatientNumber + strek + PatientNumber + remaining\n",
    "    ydim,xdim,zdim,SUVMax,CTMax = read_info_txt(InfoTextFile)\n",
    "    Matrix3D = []\n",
    "    array_data = np.array(Unpack)\n",
    "    for i in range(int(zdim)):\n",
    "        Image_array = (array_data[i*int(xdim)*int(ydim):(i+1)*int(xdim)*int(ydim)]) # Creates array for each 2D image\n",
    "        ReshapeImage = np.reshape(Image_array,[int(xdim),int(ydim)]) # Reshapes image to 2D\n",
    "        if Image == 'CT':\n",
    "            CT_array = ReshapeImage*CTMax/(2**16-1)\n",
    "            Matrix3D.append(CT_array)\n",
    "        else:\n",
    "            PET_array = ReshapeImage*SUVMax/(2**16-1)\n",
    "            Matrix3D.append(PET_array)\n",
    "        Matrix3D_array = np.array(Matrix3D) # Create an array\n",
    "        Matrix3D_new = np.squeeze(Matrix3D_array) # Get 3D matrix \n",
    "    return Matrix3D_new\n",
    "\n",
    "Image_Matrix = read_CT_PET('P005','PT')\n",
    "\n",
    "def plot(Layer):\n",
    "    Layer_Image = Image_Matrix[Layer]\n",
    "    plot = pyplot.imshow(Layer_Image)# Display 2D image\n",
    "    return plot    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(Layer):\n",
    "    Layer_Image = Image_Matrix[Layer]\n",
    "    plot = pyplot.imshow(Layer_Image)# Display 2D image\n",
    "    return plot \n",
    "plot(186)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read of raw_text, and extraction of z, y and x coordinate values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding Unicode Error place. We have used a 'iso-8859-15' encoding. This seems to fix the error somewhat.\n",
    "# This function reads where GTV starts from. \n",
    "def Search_GTV(file):\n",
    "    enc = 'iso-8859-15'\n",
    "    f = open(file,encoding=enc)\n",
    "    lines =f.readlines()\n",
    "    LineNumber = []\n",
    "    Header = []\n",
    "    for line_number, line in enumerate(lines):\n",
    "        if 'GTV' in line:\n",
    "            LineNumber.append(line_number)\n",
    "            Header.append(line)\n",
    "    return LineNumber,Header\n",
    "\n",
    "# This is just a function that reads the file.\n",
    "def readline(file,k):\n",
    "    enc = 'iso-8859-15'\n",
    "    with open(file,encoding=enc, mode='r') as file:\n",
    "        for i in range(int(k)):\n",
    "            fileContent = file.readline()\n",
    "        filecontent = file.read()\n",
    "        return filecontent\n",
    "\n",
    "# This creates an array\n",
    "def array_creator(SplitFile):\n",
    "    array_info_txt = []\n",
    "    for i in range(len(SplitFile)):\n",
    "        try:\n",
    "            array_info_txt.append(float(SplitFile[i]))\n",
    "        except ValueError:\n",
    "            break\n",
    "    return array_info_txt\n",
    "\n",
    "# This gets out whole array\n",
    "\n",
    "def array(file):\n",
    "    Start_point,GTV_Names = Search_GTV(file)\n",
    "    filecontent = readline(file,Start_point[0]+1)\n",
    "    SplitFile = filecontent.split()\n",
    "    count = SplitFile.count('GTV')\n",
    "    Array_Whole = []\n",
    "    Position = []\n",
    "    i = 0\n",
    "    while i < count+1:\n",
    "        if len(SplitFile) == 0:\n",
    "            break\n",
    "        else:\n",
    "            Array = array_creator(SplitFile)\n",
    "            position = len(Array)/3 + 2\n",
    "            Position.append(position)\n",
    "            Array_Whole.append(Array)\n",
    "            filecontent = readline(file,Start_point[0]+1+np.sum(Position))\n",
    "            SplitFile = filecontent.split()\n",
    "        i += 1\n",
    "    return Array_Whole\n",
    "\n",
    "\n",
    "\n",
    "def Func_Concat_Header_Array_Index_PT_CT(file,Index_type='Index'):\n",
    "    LineNumber,Header = Search_GTV(file)\n",
    "    Array_Whole = array(file)\n",
    "    Array_Index_Whole = []\n",
    "    if Index_type == 'Index':\n",
    "        for i in Array_Whole:\n",
    "            Array = i[0::3]\n",
    "            Array_Index_Whole.append(Array)\n",
    "    elif Index_type == 'PT':\n",
    "        for i in Array_Whole:\n",
    "            Array = i[1::3]\n",
    "            Array_Index_Whole.append(Array)\n",
    "    elif Index_type == 'CT':\n",
    "        for i in Array_Whole:\n",
    "            Array = i[2::3]\n",
    "            Array_Index_Whole.append(Array)\n",
    "    Concat_Header_Array = []\n",
    "    for j, i in enumerate(Array_Index_Whole):\n",
    "        Concat_Header_Array.append([Header[j],i])\n",
    "    return Concat_Header_Array\n",
    "\n",
    "# Function to convert Index Values to Pixel Numbers\n",
    "# Try to find voxel on which doctor has drawn lines.\n",
    "\n",
    "def convert_index_to_pixel(PatientNumber):\n",
    "    z_index = []\n",
    "    y_index = []\n",
    "    x_index = []\n",
    "    Part_readfile = '/volumes/LaCie/Sagal/HN_ANON_09102017/Patient_Folders/'\n",
    "    strek ='/'\n",
    "    file_text = '_info.txt'\n",
    "    file_raw = '_raw_data.txt'\n",
    "    InfoTextFile = Part_readfile + PatientNumber + strek + PatientNumber + file_text\n",
    "    RawTextFile  = Part_readfile + PatientNumber + strek + PatientNumber + file_raw\n",
    "    Index_data = Func_Concat_Header_Array_Index_PT_CT(RawTextFile,Index_type='Index')\n",
    "    names = []\n",
    "    for i in Index_data:\n",
    "        for index in i:\n",
    "            if 'GTV' in index:\n",
    "                names.append(index)\n",
    "                continue\n",
    "            else:\n",
    "                ydim,xdim,zdim,SUVMax,CTMax = read_info_txt(InfoTextFile)\n",
    "                z = np.floor(np.array(index)/(xdim*ydim))\n",
    "                y = np.floor((np.array(index)-z*xdim*ydim)/(ydim))\n",
    "                x = np.array(index) - z*xdim*ydim-y*ydim\n",
    "            z_index.append(z)\n",
    "            y_index.append(y)\n",
    "            x_index.append(x)\n",
    "    return z_index,y_index,x_index,names,SUVMax\n",
    "# Husk du har lagt til SUVMax. Kan hende du opplever utfordringer når du kjører gammel kode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a function that reads out the correct tumor. We are intrested in GTV PET."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in GTV names can be absolutely improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in GTV names from excel file\n",
    "\n",
    "# Code reading in GTV names together with Patient Number.\n",
    "df = pd.read_excel('GTVNames.xlsx',skiprows=14) # Cod reading in GTV names that shall be used. \n",
    "GTV = df.iloc[:,2] # Chosing correct column\n",
    "GTVNames = GTV.to_numpy() # Convert to Numpy array\n",
    "\n",
    "#Function reading out correct Tumor.\n",
    "def CorrectTumorChoice(PatientNumber):\n",
    "\n",
    "    #array_names = ['GTV prim tum CT (70 Gy) ','GTV/CT prim tum ( 70 Gy)','GTV prim tum CT (70 Gy) ','GTV CT (70 Gy)','GTVtumor/pb ','GTVtumor(70 gy)/henma ','GTVtumor/AN ','GTV/JFE (70 Gy) ','GTVtumor(70gy)/henma','GTV_PET_TUMOR ','GTV_Tumor_PET ','GTVtumor','GTV tumor/brt ','GTVvetonsille/henma ','GTV tumor 70 Gy- uxjolp ','GTV tumor (70 Gy) - uxjolp','GTV_PET_TUMOR (70 Gy) ','GTV prim tum PET/CT (70 Gy) ','GTV_tumor_PET_TVB ','GTV_TUMOR_PET_TVB ','GTV_PET_TUMOR_TVB ','GTV_PET_tumor_TVB ','GTV_PET_TVB ','GTV_PET_tvb ','GTV PET (70 Gy) ','GTV PET JFE (70 Gy) ','GTV primtum PET (70 Gy) ','GTV tumor PET CT','GTV_PET_tumor ','GTV_tumor_PET_tvb ','GTV_tumor_PET_AL','GTV_LNSIN_PET_TVB ','GTV_tumor_PET (70 Gy) ','GTV_tumor_PET ','GTV prim tum PET/CT,GTV prim tum PET/CT (70 Gy)','GTV PET prim tum (70 Gy)','GTV prim tum PET (70 Gy)','GTV prim tum PET','GTV prim tum PET ()70 Gy)', 'GTV_PET_TUMOR (70 Gy)','GTV_PRIMTUM_PET (70 Gy)','GTV CT','GTV PET (70 Gy)' ,'GTV prim tum (70 Gy)','GTV prim tum PET','GTV prim tum MR','GTV prim tum CT (70 Gy)' ,'GTV prim tum CT (70 Gy)','GTV tumor 70GY/MN','GTV_TUMOR_PET hs_TVB','GTV_tumor_PET_tvb','GTV primtum PET JFE (70 Gy)','GTV_TUMOR_PET_TVB','GTV_PET_tumor_TVB','GTV_TUMOR?_PET_TVB','GTV_TUMOR_PET_TVB 70 Gy' ,'GTV_PET_tumor_TVB','GTV_PET_TUMOR_TVB','GTV_tumor_PET_TVB' ,'GTV_tumor_PET_TVB (70 Gy)','GTV-PET l.gl ve 1/brt' ,'GTV-PET lgl ve2/brt','GTV tonsille (70 Gy)/pbk','GTV prim tum (68 Gy)','GTV_TUMOR_PET ','GTV prim tum','GTV_LNdex_PET','GTV_origo?_PET']\n",
    "    array_names = GTVNames\n",
    "    PatientWithTumor=[]\n",
    "    if PatientNumber == 'P118':\n",
    "        pass\n",
    "    else:\n",
    "        Z,Y,X,names,SUVMax = convert_index_to_pixel(PatientNumber)\n",
    "    KO = []\n",
    "    Names = []\n",
    "    results = [] # Will contain array name, and Array number later.\n",
    "    for i in names:\n",
    "        string = i.strip('\\n')\n",
    "        Names.append(string)\n",
    "    for i in range(len(Names)):\n",
    "        Namy = Names[i]\n",
    "        for k in array_names:\n",
    "            if Namy == k:\n",
    "                KO.append(Namy)\n",
    "                KO.append(i)\n",
    "                PatientWithTumor.append(PatientNumber)\n",
    "                return Namy,i,PatientWithTumor\n",
    "    empty_array = np.array(KO) # Brude denne snipp biten til å legge igjen de jeg ikke finner Tumor for. \n",
    "    if empty_array.size == 0:\n",
    "        pass\n",
    "        return print('No Tumor Found'),print(Names),print(PatientNumber)\n",
    "\n",
    "\n",
    "\n",
    "# Function that creates patient list\n",
    "def PatientList(Number):\n",
    "    Patient_List = []\n",
    "    PossibleAvailable = []\n",
    "    for i in range(1,Number+1):\n",
    "        PossibleAvailable.append(i)\n",
    "        Standard = 'P'\n",
    "        OneZero = '0'\n",
    "        TwoZer0 = '00'\n",
    "        digits = int(math.log10(i))+1 # Use this to code number of digits of 'i'\n",
    "        if digits == 1:\n",
    "            PatientNum = Standard + TwoZer0 + str(i)\n",
    "        elif digits == 2:\n",
    "            PatientNum = Standard + OneZero + str(i)\n",
    "        else:\n",
    "            PatientNum = Standard + str(i)\n",
    "        Patient_List.append(PatientNum)\n",
    "    return Patient_List\n",
    "\n",
    "# In number you incert number of patients you want to check out\n",
    "def Check_Funtion(Number):\n",
    "    #Create a list with PET values for patients:-\n",
    "    NumberAvailable = []\n",
    "    UniCodeDecodeErrorAntall = []\n",
    "    AbsentFilesNumber = []\n",
    "    for i in (PatientList(Number)):\n",
    "        try:\n",
    "            GTVName,GTVPositionNumber,PatientWithTumor = CorrectTumorChoice(i)\n",
    "            print(i,GTVName,GTVPositionNumber)\n",
    "            if PatientWithTumor == None:\n",
    "                pass\n",
    "            else:\n",
    "                NumberAvailable.append(PatientWithTumor)\n",
    "        except FileNotFoundError:\n",
    "            print(i,'This file is absent')\n",
    "            AbsentFilesNumber.append(i)\n",
    "        except TypeError:\n",
    "            pass\n",
    "        except UnboundLocalError:\n",
    "            pass\n",
    "    print('Number Available are',len(NumberAvailable))\n",
    "    print('The Possible Available are',Number)\n",
    "    print('Absent files are',len(AbsentFilesNumber))\n",
    "    return NumberAvailable,AbsentFilesNumber\n",
    "# Pasient nr 180 intressant. Den har flere prim tumor.\n",
    "# Pasient nr 160 er jeg usikker på.\n",
    "\n",
    "PatientWithTumors,AbsentFiles=(Check_Funtion(254))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creates a 3D matrix that icludes tumor voxel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates a 3D matrix that contains whole tumor. Also coordinates that lineates around the tumor is included. \n",
    "# In the code we use function CorrectTumorChoice. This choses for us the correct tumor.\n",
    "def create_dummy_matrix(PatientNumber):\n",
    "    pixel = convert_index_to_pixel(PatientNumber)\n",
    "    z,y,x,names,suvmax = pixel\n",
    "    Part_readfile = '/volumes/LaCie/Sagal/HN_ANON_09102017/Patient_Folders/'\n",
    "    strek ='/'\n",
    "    file_text = '_info.txt'\n",
    "    file_raw = '_raw_data.txt'\n",
    "    InfoTextFile = Part_readfile + PatientNumber + strek + PatientNumber + file_text\n",
    "    ydim,xdim,zdim,SUVMax,CTMax = read_info_txt(InfoTextFile)\n",
    "    z,y,x,name,suvmax = pixel\n",
    "    try:\n",
    "        ArrayName,ArrayNum,PatientWithTumor = CorrectTumorChoice(PatientNumber)\n",
    "        z = (z[ArrayNum])# RART! Hvorfor ikke bare z?\n",
    "        y = (y[ArrayNum]) # Rart! Hvorfor ikke bare y?\n",
    "        x = (x[ArrayNum]) # RART!\n",
    "        Dummy = np.zeros([int(zdim),int(xdim),int(ydim)])\n",
    "        X_axis = []\n",
    "        Y_axis = []\n",
    "        Z_axis = []\n",
    "        for i in range(len(z)):\n",
    "            x1 = int(x[i])\n",
    "            y1 = int(y[i])\n",
    "            z1 = int(z[i])\n",
    "            X_axis.append(x1)\n",
    "            Y_axis.append(y1)\n",
    "            Z_axis.append(z1)\n",
    "            Dummy[z1,x1,y1] = 1000\n",
    "        return Dummy,X_axis,Y_axis,Z_axis\n",
    "    except TypeError:\n",
    "        pass\n",
    "\n",
    "# Function below visualizes in 2D\n",
    "def PLOT_DUMMY(PATIENTnuMB):\n",
    "    Dummy,X,Y,Z = create_dummy_matrix(PATIENTnuMB)\n",
    "    y3 = np.array(Y)\n",
    "    x3 = np.array(X)\n",
    "    z3 = np.array(Z)\n",
    "    Center_x =(np.amax(X)+np.amin(X))/2\n",
    "    Center_y =(np.amax(Y)+np.amin(Y))/2\n",
    "    Center_z =(np.amax(Z)+np.amin(Z))/2\n",
    "    print(Center_x,Center_y,Center_z)\n",
    "    fig = pyplot.figure()\n",
    "    pyplot.plot(x3,y3,'*')\n",
    "    pyplot.show()\n",
    "    # Function below visualizes tumor in 3D\n",
    "    from itertools import product, combinations\n",
    "    from matplotlib import pyplot as plt\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "    pyplot.rcParams[\"figure.figsize\"] = [7.00, 3.50]\n",
    "    pyplot.rcParams[\"figure.autolayout\"] = True\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(x3, y3, z3, c=z3, alpha=1)\n",
    "    pyplot.show()\n",
    "'''    Image_Matrix = read_CT_PET(PATIENTnuMB,'PT')\n",
    "    Layer_Image = Image_Matrix[int(Center_z)]\n",
    "    fig = plt.figure()\n",
    "    pyplot.imshow(Layer_Image)'''# Display 2D image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate various PET parameters for ALL patients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to collect SUV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function extracting SUVVALUES \n",
    "# From it we also obtain the Tumor volume.\n",
    "\n",
    "def SUVExtractor(PatientNumber):\n",
    "    PatientWithTumor = []  # Here I store patient with a primary tumor. \n",
    "    Part_readfile = '/volumes/LaCie/Sagal/HN_ANON_09102017/Patient_Folders/'\n",
    "    strek ='/'\n",
    "    file_raw = '_raw_data.txt'\n",
    "    RawTextFile  = Part_readfile + PatientNumber + strek + PatientNumber + file_raw # Read in file\n",
    "    Index_data = Func_Concat_Header_Array_Index_PT_CT(RawTextFile,Index_type='PT') # Extract index\n",
    "    PET_VALUES = []\n",
    "    for i in Index_data:\n",
    "        for j in i:\n",
    "            if j == 'Hi':\n",
    "                pass\n",
    "            else:\n",
    "                PET_VALUES.append(j)\n",
    "    # To chose correct array number FROM PET_VALUES, ARRAY NUM IS USED.\n",
    "    try:\n",
    "        ArrayName,ArrayNum,PatientWithTumor = CorrectTumorChoice(PatientNumber)     \n",
    "        # Need to define a so that we chose the correct array\n",
    "        a = 2*ArrayNum +1\n",
    "        Pet_Values0 = PET_VALUES[a]\n",
    "        PatientWithTumor.append(PatientNumber)\n",
    "        return Pet_Values0\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "# Lese ut tumor verdi.\n",
    "def SUV_samler(PatientName):\n",
    "    SUVSAMLET = []\n",
    "    for i in (PatientName):\n",
    "        if i == 'P118': # Jumping over because it causes code to crash\n",
    "            pass\n",
    "        else:\n",
    "            b = (SUVExtractor(i))\n",
    "            a = np.array(b)\n",
    "            SUVSAMLET.append(a)\n",
    "    return SUVSAMLET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to calculate PET parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract SUVMEAN from SUV-values obtained from files. \n",
    "def SUVMEAN(SUV):\n",
    "    SUVMEANLIST = []\n",
    "    for i in SUV:\n",
    "        a = np.mean(i)\n",
    "        SUVMEANLIST.append(a)\n",
    "    return SUVMEANLIST\n",
    "\n",
    "# Extract SUVMAX from SUV-values obtained from files. \n",
    "\n",
    "def SUVMAXMINE(SUV):\n",
    "    SUVMAXLIST = []\n",
    "    for i in SUV:\n",
    "        a = np.amax(i)\n",
    "        SUVMAXLIST.append(a)\n",
    "    return SUVMAXLIST\n",
    "\n",
    "#Tumor volume from the length of the PET-values for each pasient. \n",
    "def TumorVolumeFunction(SUV):\n",
    "    TumorVolume = []\n",
    "    PatientNumber = []\n",
    "    for i in SUV:\n",
    "        a = len(i)/1000\n",
    "        TumorVolume.append(a)\n",
    "    return TumorVolume\n",
    "\n",
    "\n",
    "def MTV(SUV):\n",
    "    MTV = []\n",
    "    SuvMaximum = SUVMAXMINE(SUV) # Maximum SUV for patients are stored here. \n",
    "    SUVPixelValue = [] # SUV-values greater than threshold are collected here. \n",
    "    SUVCOLLECTER = []\n",
    "    for k, i in enumerate (SUV):\n",
    "        SuvMaX  = SuvMaximum[k]\n",
    "        Threshold = SuvMaX*0.41\n",
    "        if Threshold < 2.5:\n",
    "            Threshold = 2.5\n",
    "        for j in  i:\n",
    "            if j > Threshold:\n",
    "                SUVPixelValue.append(j)\n",
    "        SUVCOLLECTER.append(SUVPixelValue)\n",
    "        SUVPixelValue = []\n",
    "\n",
    "    for i in SUVCOLLECTER:\n",
    "        if i == []:\n",
    "            pass\n",
    "        else:\n",
    "            a = len(i)/1000\n",
    "            MTV.append(a)\n",
    "    return MTV\n",
    "\n",
    "\n",
    "def TLG(SUV):\n",
    "    SuvMean=SUVMEAN(SUV)\n",
    "    TLG = []\n",
    "    for i,j in enumerate (MTV(SUV)):\n",
    "        tlg = j*SuvMean[i]\n",
    "        TLG.append(tlg)\n",
    "    return TLG\n",
    "\n",
    "def center_SUV(SUV,PatientGroup):\n",
    "    Index = find_Center_Index(PatientGroup)\n",
    "    Centerr_SUV=[]\n",
    "    for i,j in enumerate (SUV):\n",
    "        centerr=Index[i]\n",
    "        Centerr=SUV[centerr]\n",
    "        Centerr_SUV.append(Center)\n",
    "        return Centerr_SUV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in all patients and HPV positive and negative patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that stores all patient groups.\n",
    "def allpatients():\n",
    "    AllPatients = []\n",
    "    for i in PatientWithTumors:\n",
    "        for j in i:\n",
    "            AllPatients.append(j)\n",
    "    return AllPatients\n",
    "\n",
    "# Function used to read in HPV file.\n",
    "def ReadHPVSituation():\n",
    "    df = pd.read_csv('hpv_group_all.csv',skiprows=0)\n",
    "    a = df.iloc[:,1]\n",
    "    HPV_Situation = a.to_numpy()\n",
    "    b = df.iloc[:,0]\n",
    "    NumberOfPatient=b.to_numpy()\n",
    "    return HPV_Situation,NumberOfPatient\n",
    "\n",
    "# Read out patientnumber from file\n",
    "def HPVPatientNumber():\n",
    "    HPV_Situation,NumberOfPatient = ReadHPVSituation()\n",
    "    # Grupperer pasienter i HPV postive = 1, og negative = 0. \n",
    "    PatientnumbOne = []\n",
    "    PatientnumbZero = []\n",
    "    HPVSITUATION = []\n",
    "\n",
    "    for i,j in enumerate(HPV_Situation):\n",
    "        if j == 0:\n",
    "            PatientnumbZero.append(NumberOfPatient[i])\n",
    "        elif j == 1:\n",
    "            HPVSITUATION.append(j)\n",
    "            PatientnumbOne.append(NumberOfPatient[i])\n",
    "    return PatientnumbZero,PatientnumbOne\n",
    "\n",
    "PatientnumbZero,PatientnumbOne = HPVPatientNumber()\n",
    "\n",
    "# Code that Adds P and necessary zeros before patient id\n",
    "def ExtractionPationNumbersHPV(HPVPatientsRelation):\n",
    "    List = []\n",
    "    for i in HPVPatientsRelation:\n",
    "        Standard = 'P'\n",
    "        OneZero = '0'\n",
    "        TwoZer0 = '00'\n",
    "        a = int(i)\n",
    "        digits = int(math.log10(a))+1 # Use this to code number of digits of 'i'\n",
    "        if digits == 1:\n",
    "            PatientNum = Standard + TwoZer0 + str(a)\n",
    "        elif digits == 2:\n",
    "            PatientNum = Standard + OneZero + str(a)\n",
    "        else:\n",
    "            PatientNum = Standard + str(a)\n",
    "        List.append(PatientNum)\n",
    "\n",
    "    return (List)\n",
    "\n",
    "# Function removing patients that do not have a GTV\n",
    "def FinalPatientNumberHPV(PatientnumbZero):\n",
    "    PatientNumberHPV0list = ExtractionPationNumbersHPV(PatientnumbZero)\n",
    "    PatientNumberList=[]\n",
    "    PatientWithTumor = allpatients()\n",
    "    for i in PatientWithTumor:\n",
    "        for k in PatientNumberHPV0list:\n",
    "            if k == i:\n",
    "                PatientNumberList.append(i)\n",
    "    return PatientNumberList\n",
    "\n",
    "\n",
    "#PatientNumber for HPV negative pasients are saved here.\n",
    "PatientNumbeRHPVZero = FinalPatientNumberHPV(PatientnumbZero)\n",
    "#PatientNumber for HPV positive pasients are saved here.\n",
    "PatientNumbeRHPVOne = FinalPatientNumberHPV(PatientnumbOne)\n",
    "#All pasients are saved here\n",
    "AllPasientNumbers = allpatients()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract outcome, and also corresponding patient numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that reads inn files:\n",
    "# ArrayNumber if pasient is not sick\n",
    "Sykdomsfri = 90\n",
    "#ArrayNumber if pasient survies\n",
    "Survival = 79\n",
    "#Alder\n",
    "Ecog = 32\n",
    "#MTV\n",
    "MTV = 59\n",
    "def ReadInFiles(ArrayNumber):\n",
    "    #Read in Data\n",
    "    df = pd.read_excel('pasinfo.xls',skiprows=2)\n",
    "    # Reading in Patient ID\n",
    "    ab = df.iloc[:,0]\n",
    "    # Read in over all survival.\n",
    "    a = df.iloc[:,ArrayNumber]\n",
    "    # Remove Nan from list\n",
    "    #Pasient = ab[:254]\n",
    "    #PasientId = ab[np.logical_not(np.isnan(ab))]\n",
    "    PasientId = ab[:254]\n",
    "    # Remove Nan from list\n",
    "    OverAllSurvival = a[:254]\n",
    "    #OverAllSurvival = a[np.logical_not(np.isnan(a))]\n",
    "    return PasientId,OverAllSurvival\n",
    "\n",
    "\n",
    "PasientID,OverAllSurvival=ReadInFiles(Survival)\n",
    "\n",
    "\n",
    "# Function to create Readable pasient numbers from PasientID\n",
    "\n",
    "def Patien_List():\n",
    "    Patient_List=[]\n",
    "    for i in PasientID:\n",
    "        Standard = 'P'\n",
    "        OneZero = '0'\n",
    "        TwoZer0 = '00'\n",
    "        a = int(i)\n",
    "        digits = int(math.log10(a))+1 # Use this to code number of digits of 'i'\n",
    "        if digits == 1:\n",
    "            PatientNum = Standard + TwoZer0 + str(a)\n",
    "        elif digits == 2:\n",
    "            PatientNum = Standard + OneZero + str(a)\n",
    "        else:\n",
    "            PatientNum = Standard + str(a)\n",
    "        Patient_List.append(PatientNum)\n",
    "    return Patient_List\n",
    "\n",
    "\n",
    "# Create a function that can take in different groups of Pasient.\n",
    "# Compare it to what is available in file\n",
    "# Extract Outcome\n",
    "\n",
    "def OutComeOfPatientList(PatientGroup):\n",
    "    Utfall = []\n",
    "    Pasienter = []\n",
    "    Patient_List = Patien_List()\n",
    "    for j in PatientGroup:\n",
    "        for i,k in enumerate (Patient_List):\n",
    "            if k == j:\n",
    "                Pasienter.append(k)\n",
    "                U = OverAllSurvival[i]\n",
    "                Utfall.append(U)\n",
    "    return Utfall,Pasienter\n",
    "\n",
    "PasientID,OverAllSurvival=ReadInFiles(Ecog)\n",
    "\n",
    "\n",
    "# ECOG, og pasient nummer for 3 pasient grupper\n",
    "ECOG_ALL,PasietNumbAll = OutComeOfPatientList(AllPasientNumbers)\n",
    "ECOG_HPVPOS,PasietNumbPOS = OutComeOfPatientList(PatientNumbeRHPVOne)\n",
    "ECOG_HPVNEG,PasietNumbNEG = OutComeOfPatientList(PatientNumbeRHPVZero)\n",
    "\n",
    "PasientID,OverAllSurvival=ReadInFiles(MTV)\n",
    "\n",
    "\n",
    "#MTV-U fra filer.\n",
    "MTV_ALL_O,PasietNumbAll = OutComeOfPatientList(AllPasientNumbers)\n",
    "MTV_HPVPOS_O,PasietNumbPOS = OutComeOfPatientList(PatientNumbeRHPVOne)\n",
    "MTV_HPVNEG_O,PasietNumbNEG = OutComeOfPatientList(PatientNumbeRHPVZero)\n",
    "\n",
    "MTV_FILE = []\n",
    "PatientNum=[]\n",
    "PasietNumbNEG_File = []\n",
    "for j,i in enumerate (MTV_HPVNEG_O):\n",
    "    if np.isnan(i):\n",
    "        pass\n",
    "    else:\n",
    "        MTV_FILE.append(i)\n",
    "        PatientNum.append(PatientNumbeRHPVZero[j])\n",
    "\n",
    "PasientID,OverAllSurvival=ReadInFiles(Survival)\n",
    "Outcome_file,PasietNumbNEG_file = OutComeOfPatientList(PatientNum) \n",
    "#SUV_HPVNEG_file = SUV_samler(PasietNumbNEG_file)\n",
    "print(Outcome_file)\n",
    "\n",
    "\n",
    "PasientID,OverAllSurvival=ReadInFiles(Survival)\n",
    "# Utfall, og pasient nummer for 3 pasient grupper\n",
    "Outcome_ALL,PasietNumbAll = OutComeOfPatientList(AllPasientNumbers)\n",
    "Outcome_HPVPOS,PasietNumbPOS = OutComeOfPatientList(PatientNumbeRHPVOne)\n",
    "Outcome_HPVNEG,PasietNumbNEG = OutComeOfPatientList(PatientNumbeRHPVZero)\n",
    "'''# Extract SUV for 3 pasient groups\n",
    "SUV_ALL = SUV_samler(PasietNumbAll)\n",
    "SUV_HPVPOS = SUV_samler(PasietNumbPOS)\n",
    "SUV_HPVNEG = SUV_samler(PasietNumbNEG)'''\n",
    "\n",
    "%store  SUV_ALL\n",
    "%store  SUV_HPVPOS\n",
    "%store  SUV_HPVNEG\n",
    "%store Outcome_HPVNEG\n",
    "%store Outcome_ALL\n",
    "%store Outcome_HPVPOS\n",
    "%store SUV_HPVNEG_file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Tumor Volume for 3 pasient groups\n",
    "TumorVolume_ALL = TumorVolumeFunction(SUV_ALL)\n",
    "TumorVolume_POS = TumorVolumeFunction(SUV_HPVPOS)\n",
    "TumorVolume_NEG = TumorVolumeFunction(SUV_HPVNEG)\n",
    "\n",
    "# Calculate Max SUV for 3 pasient groups\n",
    "SUVMAX_ALL = SUVMAXMINE(SUV_ALL)\n",
    "SUVMAX_POS = SUVMAXMINE(SUV_HPVPOS)\n",
    "SUVMAX_NEG = SUVMAXMINE(SUV_HPVNEG)\n",
    "\n",
    "#Calculate MTV FOR 3 PasientGroups\n",
    "MTV_ALL = MTV(SUV_ALL)\n",
    "MTV_POS = MTV(SUV_HPVPOS)\n",
    "MTV_NEG = MTV(SUV_HPVNEG)\n",
    "\n",
    "#Calculate TLG FOR 3 PasientGroups\n",
    "TLG_ALL = TLG(SUV_ALL)\n",
    "TLG_POS = TLG(SUV_HPVPOS)\n",
    "TLG_NEG = TLG(SUV_HPVNEG)\n",
    "\n",
    "# Ecog\n",
    "ECOG_ALL = ECOG_ALL\n",
    "ECOG_POS = ECOG_HPVPOS\n",
    "ECOG_NEG = ECOG_HPVNEG\n",
    "\n",
    "# MTV file\n",
    "MTV_NEG_file = MTV(SUV_HPVNEG_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GTV and MTV predictors are saved for 3 pasient groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tumor Volume predictors for 3 pasient groups\n",
    "X_ALL_GTV = np.array(np.ravel(TumorVolume_ALL).reshape(-1,1))\n",
    "X_HPVPOS_GTV = np.array(np.ravel(TumorVolume_POS).reshape(-1,1))\n",
    "X_HPVNEG_GTV = np.array(np.ravel(TumorVolume_NEG).reshape(-1,1))\n",
    "# MTV predictors for 3 pasient groups\n",
    "X_ALL_MTV = np.array(np.ravel(MTV_ALL).reshape(-1,1))\n",
    "X_HPVPOS_MTV = np.array(np.ravel(MTV_POS).reshape(-1,1))\n",
    "X_HPVNEG_MTV = np.array(np.ravel(MTV_NEG).reshape(-1,1))\n",
    "# TLG predictors for 3 pasient groups\n",
    "X_ALL_TLG = np.array(np.ravel(TLG_ALL).reshape(-1,1))\n",
    "X_HPVPOS_TLG = np.array(np.ravel(TLG_POS).reshape(-1,1))\n",
    "X_HPVNEG_TLG = np.array(np.ravel(TLG_NEG).reshape(-1,1))\n",
    "# SUVMAX predictors for 3 pasient groups\n",
    "X_ALL_SUVMAX = np.array(np.ravel(SUVMAX_ALL).reshape(-1,1))\n",
    "X_HPVPOS_SUVMAX = np.array(np.ravel(SUVMAX_POS).reshape(-1,1))\n",
    "X_HPVNEG_SUVMAX = np.array(np.ravel(SUVMAX_NEG).reshape(-1,1))\n",
    "# Ecog predictors for 3 pasient groups\n",
    "X_ALL_ECOG = np.array(np.ravel(ECOG_ALL).reshape(-1,1))\n",
    "X_HPVPOS_ECOG = np.array(np.ravel(ECOG_POS).reshape(-1,1))\n",
    "X_HPVNEG_ECOG = np.array(np.ravel(ECOG_NEG).reshape(-1,1))\n",
    "\n",
    "# MTV file\n",
    "X_HPVNEG_MTV_file = np.array(np.ravel(MTV_NEG_file).reshape(-1,1))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that extracts patient coordinates\n",
    "# Here we run through an array for x,y and z\n",
    "# I choose those that fullfil x**2+y**2+z**2 = r**2.\n",
    "def CircleCoordinates(radiusLength):\n",
    "    # Create arrays for x,y and z\n",
    "    y = np.linspace(-radiusLength,radiusLength,2*radiusLength+1)\n",
    "    z = np.linspace(-radiusLength,radiusLength,2*radiusLength+1)\n",
    "    x = np.linspace(-radiusLength,radiusLength,2*radiusLength+1)\n",
    "    # Loop over and get possible whole coordinates from circle arrays x,y and z\n",
    "    o=[] # Collects x,y and z that fullfil r**2=x**2+y**2+z**2\n",
    "    P=[] # Collcects the coordinates, o.\n",
    "    for i in range(radiusLength):\n",
    "        for j in y:\n",
    "            for k in x:\n",
    "                for p in z:\n",
    "                    if j**2+k**2+p**2 == i**2:\n",
    "                        o.append(j)\n",
    "                        o.append(k)\n",
    "                        o.append(p)\n",
    "        P.append(o)\n",
    "        o=[]\n",
    "    # Group the different coordinates for each radius in 3 and 3.\n",
    "    Final=[]\n",
    "    for i in P:\n",
    "        N = 3\n",
    "        subList = [i[n:n+N] for n in range(0, len(i), N)]\n",
    "        Final.append(subList)\n",
    "    return Final\n",
    "# Make one function that extracts information about Patient. This cuts down computation time\n",
    "def CenterXYZ(PatientNumber):\n",
    "    Dummy,X,Y,Z = create_dummy_matrix(PatientNumber)\n",
    "    Center_x =(np.amax(X)+np.amin(X))/2\n",
    "    Center_y =(np.amax(Y)+np.amin(Y))/2\n",
    "    Center_z =(np.amax(Z)+np.amin(Z))/2\n",
    "    return Center_x,Center_y,Center_z,X,Y,Z\n",
    "\n",
    "\n",
    "# Create a function that adds center of tumor\n",
    "def Radius_Array(Center_x,Center_y,Center_z,Array):\n",
    "    Ko = []\n",
    "    for i,j in enumerate(Array):\n",
    "        x = j[0]+int(Center_x)-1\n",
    "        y = j[1]+int(Center_y)-1\n",
    "        z = j[2]+int(Center_z)-1\n",
    "        Ko.append(x)\n",
    "        Ko.append(y)\n",
    "        Ko.append(z)\n",
    "    N = 3\n",
    "    SubList = [Ko[n:n+N] for n in range(0, len(Ko), N)]\n",
    "\n",
    "    return SubList\n",
    "\n",
    "# This function finds out WHERE the sphere and the tumor actualy do intersect.\n",
    "# This is done for every radius.\n",
    "def intersect(array,X,Y,Z):\n",
    "    index_list = []\n",
    "    for i, j in enumerate(Z):\n",
    "        x_values = X[i] # x-coordinate\n",
    "        y_values = Y[i] # y-coordinate\n",
    "        for å in array:#\n",
    "            if å[0] == x_values and å[1] == y_values and å[2] == j:\n",
    "                    index_list.append(i)\n",
    "    return index_list\n",
    "\n",
    "\n",
    "# This function runs over all sphere radiuses and adds tumor center using function radius_array\n",
    "# Then it runs over all radius, and finds the intersection point each radius has with tumor.\n",
    "# This is stored in array index_radiuses.\n",
    "def RadiusTumorCenter(RadiusLength,PatientNumber):\n",
    "    Center_x,Center_y,Center_z,X,Y,Z = CenterXYZ(PatientNumber)\n",
    "    Sphere_Coordinates = CircleCoordinates(RadiusLength)\n",
    "    AddedTumorCenter=[]\n",
    "    for i in Sphere_Coordinates:\n",
    "        a = Radius_Array(Center_x,Center_y,Center_z,i)\n",
    "        AddedTumorCenter.append(a)\n",
    "    Index_Radiuses=[]\n",
    "    for i in AddedTumorCenter:\n",
    "        Index_Radius =  intersect(i,X,Y,Z)\n",
    "        Index_Radiuses.append(Index_Radius)\n",
    "    return Index_Radiuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that calculates mean SUV for EACH radius.\n",
    "def Pet_mean_for_each_radius(Radius_Arrayy,Pet_Values):\n",
    "    PET_values_intersections = []\n",
    "    for i in Radius_Arrayy:\n",
    "        u = Pet_Values[i]\n",
    "        PET_values_intersections.append(u)\n",
    "    MEANPET = np.mean(PET_values_intersections)\n",
    "    return MEANPET\n",
    "\n",
    "# Takes in the indexes given by RadiusTumorCenter function\n",
    "# Brings out the PET values with those indexes, and calculate mean PET for every sphere going out.\n",
    "def Read_Extract_PET(PatientNumber,Length_Of_Radius):\n",
    "    # Data is read in\n",
    "    Part_readfile = '/volumes/LaCie/Sagal/HN_ANON_09102017/Patient_Folders/'\n",
    "    strek ='/'\n",
    "    file_text = '_info.txt'\n",
    "    file_raw = '_raw_data.txt'\n",
    "    InfoTextFile = Part_readfile + PatientNumber + strek + PatientNumber + file_text\n",
    "    RawTextFile  = Part_readfile + PatientNumber + strek + PatientNumber + file_raw\n",
    "    Index_data = Func_Concat_Header_Array_Index_PT_CT(RawTextFile,Index_type='PT')\n",
    "    PET_VALUES = []\n",
    "    # Do not remeber. But it is important\n",
    "    for i in Index_data:\n",
    "        for j in i:\n",
    "            if j == 'Hi':\n",
    "                pass\n",
    "            else:\n",
    "                PET_VALUES.append(j)\n",
    "    # To chose correct array number FROM PET_VALUES, ARRAY NUM IS USED.\n",
    "    ArrayName,ArrayNum,PatientWithTumor = CorrectTumorChoice(PatientNumber)\n",
    "    # Need to define a so that we chose the correct array\n",
    "    a = 2*ArrayNum +1\n",
    "    Pet_Values0 = PET_VALUES[a]\n",
    "    StoredPET = []\n",
    "    Radius_Index_Array=RadiusTumorCenter(Length_Of_Radius,PatientNumber,)\n",
    "\n",
    "    for i in Radius_Index_Array:\n",
    "        b = Pet_mean_for_each_radius(i,Pet_Values0)\n",
    "        if b <1000: # Do this to exclude NaN\n",
    "            c = b\n",
    "        else:\n",
    "            c = 0\n",
    "        StoredPET.append(c)\n",
    "    return StoredPET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profile plot, and extraction of the MEAN SUV INTERCEPTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From function read_extract_pet there are zeros due to radius length included\n",
    "# This function we use to remove those.\n",
    "\n",
    "def remove_zeros_end(PatientNumber,ArrayLength):\n",
    "    PET_SUV = Read_Extract_PET(PatientNumber,ArrayLength)\n",
    "    PETSUV=[]\n",
    "    for i in PET_SUV[10:]:\n",
    "        if i == 0:\n",
    "            pass\n",
    "        else:\n",
    "            PETSUV.append(i)\n",
    "    a = PET_SUV[:10]\n",
    "    for i in PETSUV:\n",
    "        a.append(i)\n",
    "\n",
    "    return a  \n",
    "\n",
    "def plot_Patients_HPV_NEG(PatientNumber,PET_SUV):\n",
    "    # Y axis values\n",
    "    y = PET_SUV\n",
    "    # corresponding X axis values\n",
    "    x = np.linspace(0,len(PET_SUV)/(len(PET_SUV)),len(PET_SUV))\n",
    "\n",
    "    # plotting the points\n",
    "    plt.plot(x, y,'*')\n",
    "\n",
    "    # naming the x axis\n",
    "    plt.xlabel('Radius in pixel')\n",
    "    # naming the y axis\n",
    "    plt.ylabel('SUV')\n",
    "\n",
    "    # giving a title to my graph\n",
    "    plt.title(PatientNumber)\n",
    "\n",
    "    # function to show the plot\n",
    "    plt.show()\n",
    "    \n",
    "    return PET_SUV\n",
    "\n",
    "\n",
    "# Function below extracts PET_SUV_ALL, but we have stored it. So we dont need to run it. \n",
    "'''def plot_Patients_HPV_NEG(PatientNumber,ArrayLength):\n",
    "    PET_SUV = remove_zeros_end(PatientNumber,ArrayLength)\n",
    "    # x axis values\n",
    "    y = PET_SUV\n",
    "    # corresponding y axis values\n",
    "    x = np.linspace(0,len(PET_SUV)/(len(PET_SUV)),len(PET_SUV))\n",
    "\n",
    "\n",
    "    # plotting the points\n",
    "    plt.plot(x, y,'*')\n",
    "\n",
    "    # naming the x axis\n",
    "    plt.xlabel('Radius in pixel')\n",
    "    # naming the y axis\n",
    "    plt.ylabel('SUV')\n",
    "\n",
    "    # giving a title to my graph\n",
    "    plt.title(PatientNumber)\n",
    "\n",
    "    # function to show the plot\n",
    "    plt.show()\n",
    "    \n",
    "    return PET_SUV'''\n",
    "\n",
    "'''PET_SUV_ALL=[]\n",
    "for j,i in enumerate (PasietNumbNEG):\n",
    "    print('Outcome',Outcome_HPVNEG[j])\n",
    "    PET_SUV = plot_Patients_HPV_NEG(i,50)\n",
    "    PET_SUV_ALL.append(PET_SUV)\n",
    "# FUNCTION OVER IS USED TO STORE pet_suv'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r PET_SUV_ALL\n",
    "for j,i in enumerate(PET_SUV_ALL):\n",
    "    print('Outcome',Outcome_HPVNEG[j])\n",
    "    PatientNumber = PasietNumbNEG[j]\n",
    "    PLOT_DUMMY(PatientNumber)\n",
    "    PET_SUV = plot_Patients_HPV_NEG(PatientNumber,i)\n",
    "    print('Interpolated PET_SUV')\n",
    "    PET_SUV_INTERP= plot_Patients_HPV_NEG(PatientNumber,PET_SUV_ALL_INTERP[j])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolation of mean suv interception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run trough and interpolate PET_SUV_\n",
    "PET_SUV_ALL_INTERP=[]\n",
    "Radius = np.linspace(0,50,50)\n",
    "base_radius = np.linspace(0, 1, 101)\n",
    "for i in PET_SUV_ALL:\n",
    "    a = len(i)\n",
    "    Radius = np.linspace(0,1,a)\n",
    "    SUV_CENTER_INTERP = np.interp(base_radius, Radius, i)\n",
    "    PET_SUV_ALL_INTERP.append(SUV_CENTER_INTERP)\n",
    "\n",
    "# Here, all sphere Radius interpolated are stored.\n",
    "# I run through PET_SUV_ALL_INTERP, and only extract first radius value\n",
    "# Add it to a list, and continue until I have Sphere_Radius_all which contains SUV for each radius length for all patients. \n",
    "Sphere_Radius_all=[]\n",
    "for k in range(len(base_radius)):\n",
    "    Sphere_Radius=[]\n",
    "    for i,j in enumerate(PET_SUV_ALL_INTERP):\n",
    "        a = j[k]\n",
    "        Sphere_Radius.append(a)\n",
    "    Sphere_Radius_all.append(Sphere_Radius)\n",
    "    Sphere_Radius=[]\n",
    "\n",
    "# Here I prepare so that analysis can be performed. \n",
    "Sphere_Radius_all_processed = []\n",
    "for i in Sphere_Radius_all:\n",
    "    SphereRadius = np.array(np.ravel(i).reshape(-1,1))\n",
    "    processed_SphereRadius = np.concatenate([SphereRadius],axis=1)\n",
    "    Sphere_Radius_all_processed.append(processed_SphereRadius)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import linear_model, decomposition, datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Below gridsearch we use it to calculate Accuracy together with F1 score using a Kfold = 10\n",
    "\n",
    "'''def GridSearch(scoring,Output,split,Predictor):\n",
    "    #define cross-validation method to use\n",
    "    #cv = LeaveOneOut()\n",
    "    #define cross-validation method to use\n",
    "    cv = KFold(n_splits=split, random_state=1, shuffle=True)\n",
    "    # instantiate the model (using the default parameters)\n",
    "    logreg = LogisticRegression()\n",
    "    #logreg = SVC()\n",
    "    # fit the model with data\n",
    "    ypart = Output\n",
    "    xpart = Predictor\n",
    "    logreg.fit(xpart, ypart)\n",
    "    parameters = { \n",
    "    'C'       : np.logspace(-10,10,1000)}\n",
    "    ##parameters = { \n",
    "    #'C'       #: (C)}\n",
    "    clf = GridSearchCV(estimator = logreg,  \n",
    "                           param_grid = parameters,\n",
    "                           scoring = scoring,\n",
    "                           cv = cv)\n",
    "    clf.fit(xpart, ypart)\n",
    "    print(\"Tuned Hyperparameters :\", clf.best_params_)\n",
    "    print(\"Accuracy:\",clf.best_score_)\n",
    "    return clf.best_score_'''\n",
    "\n",
    "'''Accuracy=[]\n",
    "for i,j in enumerate(Sphere_Radius_all_processed):\n",
    "    print(i)\n",
    "    array =(GridSearch('accuracy',Outcome_HPVNEG,70,j))\n",
    "    Accuracy.append(array)\n",
    "\n",
    "F1_score=[]\n",
    "for i,j in enumerate(Sphere_Radius_all_processed):\n",
    "    print(i)\n",
    "    Array=(GridSearch('f1',Outcome_HPVNEG,10,j))\n",
    "    F1_score.append(Array)'''\n",
    "\n",
    "'''%store Accuracy\n",
    "%store F1_score'''\n",
    "\n",
    "\n",
    "'''plt.figure()\n",
    "#plt.plot(base_radius,F1_score)\n",
    "plt.figure(base_radius,Accuracy)\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# In this code, I remove those patients that include \n",
    "ko = []\n",
    "Outcome = []\n",
    "for j,i in enumerate (PET_SUV_ALL_INTERP):\n",
    "    if i[0] == 0.0 or i[1]==0.0 or i[2]==0.0 or i[3]==0.0:\n",
    "        pass\n",
    "    else:\n",
    "        ko.append(i)\n",
    "        Outcome.append(Outcome_HPVNEG[j])\n",
    "        \n",
    "\n",
    "Sphere_Radius_all=[]\n",
    "for k in range(len(base_radius)):\n",
    "    Sphere_Radius=[]\n",
    "    for i,j in enumerate(ko):\n",
    "        a = j[k]\n",
    "        Sphere_Radius.append(a)\n",
    "    Sphere_Radius_all.append(Sphere_Radius)\n",
    "    Sphere_Radius=[]\n",
    "\n",
    "# Here I prepare so that analysis can be performed. \n",
    "Sphere_Radius_all_processed = []\n",
    "for i in Sphere_Radius_all:\n",
    "    SphereRadius = np.array(np.ravel(i).reshape(-1,1))\n",
    "    processed_SphereRadius = np.concatenate([SphereRadius],axis=1)\n",
    "    Sphere_Radius_all_processed.append(processed_SphereRadius)'''\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import linear_model, decomposition, datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "def gridSearch(scoring,Output,Predictor):\n",
    "    #define cross-validation method to use\n",
    "    #cv = LeaveOneOut()\n",
    "    #define cross-validation method to use\n",
    "    # instantiate the model (using the default parameters)\n",
    "    logreg = LogisticRegression()\n",
    "    #logreg = SVC()\n",
    "    # fit the model with data\n",
    "    ypart = Output\n",
    "    xpart = Predictor\n",
    "    logreg.fit(xpart, ypart)\n",
    "    parameters = { \n",
    "    'C'       : np.logspace(-10,10,1000)}\n",
    "    ##parameters = { \n",
    "    #'C'       #: (C)}\n",
    "    clf = GridSearchCV(estimator = logreg,  \n",
    "                           param_grid = parameters,\n",
    "                           scoring = scoring)\n",
    "    clf.fit(xpart, ypart)\n",
    "    print(\"Tuned Hyperparameters :\", clf.best_params_)\n",
    "    print(\"Accuracy:\",clf.best_score_)\n",
    "    return clf.best_score_\n",
    "\n",
    "\n",
    "'''auc_score=[]\n",
    "for i,j in enumerate(Sphere_Radius_all_processed):\n",
    "    print(i)\n",
    "    Array=(gridSearch('roc_auc',Outcome,j))\n",
    "    auc_score.append(Array)'''\n",
    "\n",
    "f1_score=[]\n",
    "for i,j in enumerate(Sphere_Radius_all_processed):\n",
    "    print(i)\n",
    "    Array=(gridSearch('f1',Outcome,j))\n",
    "    f1_score.append(Array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''%store -r Accuracy\n",
    "%store -r F1_score'''\n",
    "%store f1_score\n",
    "%store auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use below code to extract index of which interpolated PET_SUV_SPHERE gives highest score\n",
    "Highest_Accuracy = 0\n",
    "for j,i in enumerate (Accuracy):\n",
    "    if Highest_Accuracy < i:\n",
    "        Highest_Accuracy = i\n",
    "        Index = j\n",
    "# Optimal Sphere is stored here\n",
    "Best_Sphere_Radius_all_processed = Sphere_Radius_all_processed[Index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure()\n",
    "plt.plot(base_radius,f1_score,label='F1_SCORE')\n",
    "plt.plot(base_radius,auc_score,label='auc_score')\n",
    "plt.plot(base_radius,Accuracy,label='Accuracy')\n",
    "plt.xlabel('Radius')\n",
    "plt.ylabel('Score')\n",
    "plt.title('F1 score vs Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split of data. Also deciding on features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# GTV DATA\n",
    "processed_Data_All_GTV = np.concatenate([X_ALL_GTV],axis=1)\n",
    "processed_Data_HPVPOS_GTV = np.concatenate([X_HPVPOS_GTV],axis=1)\n",
    "processed_Data_HPVNEG_GTV = np.concatenate([X_HPVNEG_GTV],axis=1)\n",
    "\n",
    "#SUV MAX DATA\n",
    "processed_Data_All_SUVMAX = np.concatenate([X_ALL_SUVMAX],axis=1)\n",
    "processed_Data_HPVPOS_SUVMAX = np.concatenate([X_HPVPOS_SUVMAX],axis=1)\n",
    "processed_Data_HPVNEG_SUVMAX = np.concatenate([X_HPVNEG_SUVMAX],axis=1)\n",
    "\n",
    "# MTV DATA\n",
    "processed_Data_All_MTV = np.concatenate([X_ALL_MTV],axis=1)\n",
    "processed_Data_HPVPOS_MTV = np.concatenate([X_HPVPOS_MTV],axis=1)\n",
    "processed_Data_HPVNEG_MTV = np.concatenate([X_HPVNEG_MTV],axis=1)\n",
    "\n",
    "# TLG DATA\n",
    "processed_Data_All_TLG = np.concatenate([X_ALL_TLG],axis=1)\n",
    "processed_Data_HPVPOS_TLG = np.concatenate([X_HPVPOS_TLG],axis=1)\n",
    "processed_Data_HPVNEG_TLG = np.concatenate([X_HPVNEG_TLG],axis=1)\n",
    "\n",
    "# ECOG DATA\n",
    "processed_Data_All_ECOG = np.concatenate([X_ALL_ECOG],axis=1)\n",
    "processed_Data_HPVPOS_ECOG = np.concatenate([X_HPVPOS_ECOG],axis=1)\n",
    "processed_Data_HPVNEG_ECOG = np.concatenate([X_HPVNEG_ECOG],axis=1)\n",
    "\n",
    "# MTV file\n",
    "processed_Data_HPVNEG_MTV_file = np.concatenate([X_HPVNEG_MTV_file],axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Best Best_Sphere_Radius_all_processed\n",
    "\n",
    "#Sphere_Radius = Sphere_Radius_all_processed[Index]\n",
    "\n",
    "\n",
    "def AllData(Input,Predictor):\n",
    "    if Input == 'NEG' and Predictor == 'GTV':\n",
    "        ypart = (np.ravel(Outcome_HPVNEG).reshape(-1,1))\n",
    "        xpart = (processed_Data_HPVNEG_GTV)\n",
    "    elif Input == 'NEG' and Predictor == 'MTV':\n",
    "        ypart = (np.ravel(Outcome_HPVNEG).reshape(-1,1)) \n",
    "        xpart = (processed_Data_HPVNEG_MTV)\n",
    "    if Input == 'NEG' and Predictor == 'SUVMAX':\n",
    "        ypart = (np.ravel(Outcome_HPVNEG).reshape(-1,1))\n",
    "        xpart = (processed_Data_HPVNEG_SUVMAX)\n",
    "    if Input == 'NEG' and Predictor == 'TLG':\n",
    "        ypart = (np.ravel(Outcome_HPVNEG).reshape(-1,1))\n",
    "        xpart = (processed_Data_HPVNEG_TLG)\n",
    "    if Input == 'NEG' and Predictor == 'ECOG':\n",
    "        ypart = (np.ravel(Outcome_HPVNEG).reshape(-1,1))\n",
    "        xpart = (processed_Data_HPVNEG_ECOG)\n",
    "    if Input == 'NEG_file' and Predictor == 'MTV':\n",
    "        ypart = (np.ravel(Outcome_file).reshape(-1,1))\n",
    "        xpart = (processed_Data_HPVNEG_MTV_file)\n",
    "    elif Input == 'POS' and Predictor == 'GTV' :\n",
    "        ypart = (np.ravel(Outcome_HPVPOS).reshape(-1,1))      \n",
    "        xpart = (processed_Data_HPVPOS_GTV)\n",
    "    elif Input == 'POS' and Predictor == 'MTV' :\n",
    "        ypart = (np.ravel(Outcome_HPVPOS).reshape(-1,1))        \n",
    "        xpart = (processed_Data_HPVPOS_MTV)\n",
    "    elif Input == 'POS' and Predictor == 'SUVMAX' :\n",
    "        ypart = (np.ravel(Outcome_HPVPOS).reshape(-1,1))        \n",
    "        xpart = (processed_Data_HPVPOS_SUVMAX)\n",
    "    elif Input == 'POS' and Predictor == 'TLG' :\n",
    "        ypart = (np.ravel(Outcome_HPVPOS).reshape(-1,1))        \n",
    "        xpart = (processed_Data_HPVPOS_TLG)\n",
    "    elif Input == 'POS' and Predictor == 'ECOG' :\n",
    "        ypart = (np.ravel(Outcome_HPVPOS).reshape(-1,1))        \n",
    "        xpart = (processed_Data_HPVPOS_ECOG)\n",
    "    elif Input == 'All' and Predictor == 'GTV' :\n",
    "        ypart = (np.ravel(Outcome_ALL).reshape(-1,1))\n",
    "        xpart =(processed_Data_All_GTV)\n",
    "    elif Input == 'All' and Predictor == 'MTV' :\n",
    "        ypart = (np.ravel(Outcome_ALL).reshape(-1,1))\n",
    "        xpart=(processed_Data_All_MTV)\n",
    "    elif Input == 'All' and Predictor == 'SUVMAX' :\n",
    "        ypart = (np.ravel(Outcome_ALL).reshape(-1,1))\n",
    "        xpart = (processed_Data_All_SUVMAX)\n",
    "    elif Input == 'All' and Predictor == 'TLG' :\n",
    "        ypart = (np.ravel(Outcome_ALL).reshape(-1,1))\n",
    "        xpart = (processed_Data_All_TLG)\n",
    "    elif Input == 'All' and Predictor == 'ECOG' :\n",
    "        ypart = (np.ravel(Outcome_ALL).reshape(-1,1))\n",
    "        xpart = (processed_Data_All_ECOG)\n",
    "    elif Input == 'All' and Predictor == 'SUVMAX+MTV' :\n",
    "        ypart = (np.ravel(Outcome_ALL).reshape(-1,1))\n",
    "        xpart1 = (processed_Data_All_SUVMAX)\n",
    "        xpart2 = (processed_Data_All_MTV)\n",
    "        xpart = np.concatenate((xpart1, xpart2), axis=1)\n",
    "    elif Input == 'NEG' and Predictor == 'SUVMAX+MTV' :\n",
    "        ypart = (np.ravel(Outcome_HPVNEG).reshape(-1,1))\n",
    "        xpart1 = (processed_Data_HPVNEG_SUVMAX)\n",
    "        xpart2 = (processed_Data_HPVNEG_MTV)\n",
    "        xpart = np.concatenate((xpart1, xpart2), axis=1)\n",
    "    elif Input == 'NEG' and Predictor == 'SUVMAX+GTV' :\n",
    "        ypart = (np.ravel(Outcome_HPVNEG).reshape(-1,1))\n",
    "        xpart1 = (processed_Data_HPVNEG_SUVMAX)\n",
    "        xpart2 = (processed_Data_HPVNEG_GTV)\n",
    "        xpart = np.concatenate((xpart1, xpart2), axis=1)\n",
    "    elif Input == 'NEG' and Predictor == 'SUVSphere+MTV':\n",
    "        ypart = (np.ravel(Outcome_HPVNEG).reshape(-1,1))\n",
    "        xpart1 = (processed_Data_HPVNEG_MTV)\n",
    "        xpart2 = Sphere_Radius\n",
    "        xpart = np.concatenate((xpart1, xpart2), axis=1)\n",
    "    elif Input == 'NEG' and Predictor == 'ECOG+GTV':\n",
    "        ypart = (np.ravel(Outcome_HPVNEG).reshape(-1,1))\n",
    "        xpart1 = (processed_Data_HPVNEG_GTV)\n",
    "        xpart2 = processed_Data_HPVNEG_ECOG\n",
    "        xpart = np.concatenate((xpart1, xpart2), axis=1)\n",
    "    elif Input == 'NEG' and Predictor == 'SUVCENTERinterp+MTV':\n",
    "        ypart = (np.ravel(Outcome_HPVNEG).reshape(-1,1))\n",
    "        xpart1 = (processed_Data_HPVNEG_MTV)\n",
    "        xpart2 = (processed_Data_HPVNEG_SUVCENTER_interp)\n",
    "        xpart = np.concatenate((xpart1, xpart2), axis=1)\n",
    "    elif Input == 'NEG' and Predictor == 'SUVCENTERinterp+GTV':\n",
    "        ypart = (np.ravel(Outcome_HPVNEG).reshape(-1,1))\n",
    "        xpart1 = (processed_Data_HPVNEG_GTV)\n",
    "        xpart2 = processed_Data_HPVNEG_SUVCENTER_interp\n",
    "        xpart = np.concatenate((xpart1, xpart2), axis=1)\n",
    "    elif Input == 'NEG' and Predictor == 'ECOG+MTV':\n",
    "        ypart = (np.ravel(Outcome_HPVNEG).reshape(-1,1))\n",
    "        xpart1 = (processed_Data_HPVNEG_GTV)\n",
    "        xpart2 = processed_Data_HPVNEG_ECOG\n",
    "        xpart = np.concatenate((xpart1, xpart2), axis=1)\n",
    "    elif Input == 'POS' and Predictor == 'SUVMAX+MTV' :\n",
    "        ypart = (np.ravel(Outcome_HPVPOS).reshape(-1,1))\n",
    "        xpart1 = (processed_Data_HPVPOS_SUVMAX)\n",
    "        xpart2 = (processed_Data_HPVPOS_MTV)\n",
    "        xpart = np.concatenate((xpart1, xpart2), axis=1)\n",
    "    return ypart,xpart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversikt over Data\n",
    "- Kan det være noe for HPV negative pasienter at vi kaster ut pasient med tumor = 150?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot of Logistic Function for our data. As can be seen from plots below, \n",
    "# we cant see any \"mønster.\" I will remove the smallest tumors. All those that have\n",
    "# Add your plots here, to see out. £\n",
    "\n",
    "print('HPV NEGATIVE PATIENTS')\n",
    "print('GTV')\n",
    "plt.figure(1)\n",
    "Ypart,Xpart = AllData('NEG','GTV')\n",
    "plt.plot(Xpart,Ypart,'*')\n",
    "plt.xlabel('GTV')\n",
    "plt.ylabel('Outcome')\n",
    "plt.title('GTV vs Outcome')\n",
    "plt.show()\n",
    "\n",
    "print('MTV')\n",
    "plt.figure(1)\n",
    "Ypart,Xpart = AllData('NEG','MTV')\n",
    "plt.plot(Xpart,Ypart,'*')\n",
    "plt.xlabel('MTV')\n",
    "plt.ylabel('Outcome')\n",
    "plt.title('MTV vs Outcome')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print('HPV POSITIVE PATIENTS')\n",
    "print('GTV')\n",
    "plt.figure(1)\n",
    "Ypart,Xpart = AllData('POS','GTV')\n",
    "plt.plot(Xpart,Ypart,'*')\n",
    "plt.xlabel('GTV')\n",
    "plt.ylabel('Outcome')\n",
    "plt.title('GTV vs Outcome')\n",
    "plt.show()\n",
    "\n",
    "print('MTV')\n",
    "plt.figure(1)\n",
    "Ypart,Xpart = AllData('POS','MTV')\n",
    "plt.plot(Xpart,Ypart,'*')\n",
    "plt.xlabel('MTV')\n",
    "plt.ylabel('Outcome')\n",
    "plt.title('MTV vs Outcome')\n",
    "plt.show()\n",
    "\n",
    "print('ALL PATIENTS')\n",
    "print('GTV')\n",
    "plt.figure(1)\n",
    "Ypart,Xpart = AllData('All','GTV')\n",
    "plt.plot(Xpart,Ypart,'*')\n",
    "plt.xlabel('GTV')\n",
    "plt.ylabel('Outcome')\n",
    "plt.title('GTV vs Outcome')\n",
    "plt.show()\n",
    "\n",
    "print('MTV')\n",
    "plt.figure(1)\n",
    "Ypart,Xpart = AllData('All','MTV')\n",
    "plt.plot(Xpart,Ypart,'*')\n",
    "plt.xlabel('MTV')\n",
    "plt.ylabel('Outcome')\n",
    "plt.title('MTV vs Outcome')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation between GTV and MTV for each patient group together with a plot\n",
    "- Kan se ut som korrelasjonen er størst ved de minste tumorene. Det er kanskje de største tumorene som har minst korrelasjon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.corrcoef(X_ALL_GTV.T, X_ALL_MTV.T)\n",
    "print('Correlation between GTV and MTV for whole tumor',r)\n",
    "#Plot\n",
    "plt.plot(X_ALL_GTV,X_ALL_MTV,'*')\n",
    "plt.xlabel('GTV')\n",
    "plt.ylabel('MTV')\n",
    "plt.show()\n",
    "\n",
    "r = np.corrcoef(X_HPVPOS_GTV.T, X_HPVPOS_MTV.T)\n",
    "print('Correlation between GTV and MTV for HPV POSITIVE',r)\n",
    "#Plot\n",
    "plt.figure(1)\n",
    "plt.plot(X_HPVPOS_GTV,X_HPVPOS_MTV,'*')\n",
    "plt.xlabel('GTV')\n",
    "plt.ylabel('MTV')\n",
    "plt.ylabel('GTV')\n",
    "plt.show()\n",
    "\n",
    "r = np.corrcoef(X_HPVNEG_GTV.T, X_HPVNEG_MTV.T)\n",
    "print('Correlation between GTV and MTV for HPV NEGATIVE',r)\n",
    "#Plot\n",
    "plt.plot(X_HPVNEG_GTV,X_HPVNEG_MTV,'*')\n",
    "plt.xlabel('GTV')\n",
    "plt.ylabel('MTV')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "r = np.corrcoef(Best_Sphere_Radius_all_processed.T, X_HPVNEG_MTV.T)\n",
    "print('Correlation between Best_Sphere_Radius_all and MTV for HPV NEGATIVE',r)\n",
    "#Plot\n",
    "plt.plot(Best_Sphere_Radius_all_processed,X_HPVNEG_MTV,'*')\n",
    "plt.xlabel('Best_Sphere_Radius_all')\n",
    "plt.ylabel('MTV')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOX PLOT\n",
    "- Seems like outliers are more reduced for HPV Negative patients.\n",
    "- Also outliers reduce more for MTV compared to GTV\n",
    "- Change this to Violine plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Box plot\n",
    "# Creating dataset\n",
    "np.random.seed(10)\n",
    " \n",
    "data_1 = X_ALL_GTV\n",
    "data_2 = X_ALL_MTV\n",
    "data_3 = X_HPVPOS_GTV\n",
    "data_6 = X_HPVPOS_MTV\n",
    "data_4 = X_HPVNEG_GTV\n",
    "data_5 = X_HPVNEG_MTV\n",
    "data = [data_1, data_2, data_3,data_6, data_4,data_5]\n",
    " \n",
    "fig = plt.figure(figsize =(10, 7))\n",
    " \n",
    "# Creating axes instance\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "\n",
    "ax.set_title('axes title')\n",
    " \n",
    "# Creating plot\n",
    "bp = ax.boxplot(data)\n",
    "\n",
    "plt.ylabel('Tumor Volume')\n",
    " \n",
    "# show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logit function for all three patient groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRID SEARCH\n",
    "- Noe jeg syntes er rart her er XTrain, og hele data gir samme resultat. Undersøk dette nærmere om det stemmer. Vi har fått bedre presisjon for HPV POS, og alle pasienter. Men, HPV negative er uendret. Se nærmere. \n",
    "- Se om du kan få bedre resultater om du inkluderer andre parametere enn C inverse regulirization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import linear_model, decomposition, datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def GridSearch(Input,Predictor):\n",
    "    #define cross-validation method to use\n",
    "    #cv = LeaveOneOut()\n",
    "    #define cross-validation method to use\n",
    "    cv = KFold(n_splits=65, random_state=1, shuffle=True)\n",
    "    # instantiate the model (using the default parameters)\n",
    "    logreg = LogisticRegression()\n",
    "    #logreg = SVC()\n",
    "    # fit the model with data\n",
    "    ypart,xpart = AllData(Input,Predictor)\n",
    "    logreg.fit(xpart, ypart)\n",
    "    parameters = { \n",
    "    'C'       : np.logspace(-5,5,1000)}\n",
    "    #parameters = { \n",
    "    #'C'       : (C)}\n",
    "    clf = GridSearchCV(estimator = logreg,  \n",
    "                           param_grid = parameters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = cv)\n",
    "    clf.fit(xpart, ypart)\n",
    "    print(\"Tuned Hyperparameters :\", clf.best_params_)\n",
    "    print(\"Accuracy:\",clf.best_score_)\n",
    "    return clf.best_score_\n",
    "\n",
    "\n",
    "\n",
    "# Lage Funksjon som iterer over forskjellige C-verdier.\n",
    "# Grunnet tid, skal jeg se på følgende:- MTV+suvmax,MTV+SUVCENTER,GTV+SUVCENTER\n",
    "# Function that searches through and CALCULATES AUC.\n",
    "def Score_Grid_Search(c,Outcome,Predictor):\n",
    "    scores=[]\n",
    "    C =[]\n",
    "    for k in c:\n",
    "        Ydata,Xdata = AllData(Outcome,Predictor)\n",
    "        X = np.array(Xdata) # Predictors stored here\n",
    "        y = np.array(Ydata)# Outcome\n",
    "        logreg = LogisticRegression(C = k ).fit(X, y)\n",
    "        #define cross-validation method to use\n",
    "        cv = KFold(n_splits=20, shuffle=True)\n",
    "        #use k-fold CV to evaluate model\n",
    "        score = cross_val_score(logreg, X, y, scoring='accuracy',\n",
    "                                 cv=cv, n_jobs=-1)\n",
    "        scores.append(np.mean(score))\n",
    "        C.append(c)\n",
    "    return scores,C\n",
    "\n",
    "\n",
    "def Extract_BEST_Score(c,Outcome,Predictor):\n",
    "    scores,C=Score_Grid_Search(c,Outcome,Predictor)\n",
    "    Best_Score=0\n",
    "    for i,j in enumerate (scores):\n",
    "        if j >Best_Score:\n",
    "            Best_Score=j\n",
    "            Best_C = C[i]\n",
    "    return Best_Score,Best_C\n",
    "\n",
    "# Function that searches through and CALCULATES AUC.\n",
    "def Auc_Grid_Search(c,Outcome,Predictor):   \n",
    "    AUCS=[]\n",
    "    A=[]\n",
    "    tprs =[]\n",
    "    for k in c:\n",
    "        Ydata,Xdata = AllData(Outcome,Predictor)\n",
    "        X = np.array(Xdata) # Predictors stored here\n",
    "        y = np.array(Ydata)# Outcome\n",
    "        logreg = LogisticRegression(C = k ).fit(X, y)\n",
    "        y_score = logreg.predict_proba(X)\n",
    "        fpr, tpr, _ = roc_curve(y, y_score[:, 1])\n",
    "        AUC = auc(fpr, tpr)\n",
    "        AUCS.append(AUC)\n",
    "        print(AUC)\n",
    "    return AUCS,c\n",
    "\n",
    "# Function extracting high AUC with corresponding C. \n",
    "def Extract_C_AUC(c,Outcome,Predictor):\n",
    "    AUCS,c=Auc_Grid_Search(c,Outcome,Predictor)\n",
    "    p = 0;\n",
    "    B = 0;\n",
    "    for j,i in enumerate(AUCS):\n",
    "        if i > B:\n",
    "            B = i           \n",
    "    h = []\n",
    "    p=[]\n",
    "    for i,j  in enumerate (AUCS):\n",
    "        if j >=B:\n",
    "            h.append(j)\n",
    "            o = float(c[i])\n",
    "            p.append(o)\n",
    "    return h,p\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''print('Results For All Patients GTV',GridSearch('All','GTV'))\n",
    "print('Results For All Patients MTV',GridSearch('All','MTV'))\n",
    "print('Results For All Patients SUVMAX',GridSearch('All','SUVMAX'))\n",
    "print('Results For HPV NEG GTV',GridSearch('NEG','GTV'))\n",
    "print('Results For HPV NEG MTV',GridSearch('NEG','MTV'))\n",
    "print('Results For hpv NEG  SUVMAX',GridSearch('NEG','SUVMAX'))\n",
    "print('Results For HPV POS GTV',GridSearch('POS','GTV'))\n",
    "print('Results For HPV POS MTV',GridSearch('POS','MTV'))\n",
    "print('Results For HPV POS SUVMAX',GridSearch('POS','SUVMAX'))\n",
    "print('Results For HPV NEG MTV and SuvMax',GridSearch('HPVNEG_Double','SUVMAX+MTV'))\n",
    "print('Results For HPV POS MTV and SuvMax',GridSearch('HPVPOS_Double','SUVMAX+MTV'))\n",
    "print('Results For ALL Patients MTV and SUVMax',GridSearch('All_Double','SUVMAX+MTV'))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(GridSearch('NEG_file','MTV'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "Ydata,Xdata = AllData('NEG_file','MTV')\n",
    "X = np.array(Xdata) # Predictors stored here\n",
    "y = np.array(Ydata)# Outcome\n",
    "logreg = LogisticRegression(C = 40 ).fit(X, y)\n",
    "y_score = logreg.predict_proba(X)\n",
    "fpr, tpr, _ = metrics.roc_curve(y, y_score[:, 1])\n",
    "AUC = metrics.auc(fpr, tpr)\n",
    "print(AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- Recall mot Presisjon\n",
    "- F1 score.\n",
    "- Interpolering, fjerner pasienter med spesiell tumor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All patients GridSearch HPV NEG PATIENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chose what values regularization parameter should span\n",
    "c = np.logspace(-10,10,1000)\n",
    "\n",
    "# Calculation of accuracy while using the BEST AUCS from function Extract_C_AUC()\n",
    "AUCS_GTV,C_GTV = Auc_Grid_Search(c,'NEG','MTV')\n",
    "#a,b=(GridSearch('NEG','GTV',C_GTV))\n",
    "print('Best GTV SCORE',(AUCS_GTV),'BEST C',len(C_GTV))\n",
    "'''AUCS_MTV,C_MTV = Extract_C_AUC(c,'NEG','GTV')\n",
    "cc,d=(Extract_BEST_Score(C_MTV,'NEG','MTV'))\n",
    "print('Best MTV SCORE',cc,'BEST C',d)\n",
    "AUCS_MTVSUVMAX,C_MTVSUVMAX = Extract_C_AUC(c,'HPVNEG_Double','SUVMAX+MTV')\n",
    "aa,bb=(Extract_BEST_Score(C_MTVSUVMAX,'HPVNEG_Double','SUVMAX+MTV'))\n",
    "print('Best MTV+SUVMAX SCORE',aa,'BEST C',bb)\n",
    "AUCS_GTVSUVMAX,C_GTVSUVMAX = Extract_C_AUC(c,'HPVNEG_Double','SUVMAX+GTV')\n",
    "ff,dd=(Extract_BEST_Score(C_GTVSUVMAX,'HPVNEG_Double','SUVMAX+GTV'))\n",
    "print('Best GTV+SUVMAX SCORE',ff,'BEST C',dd)\n",
    "AUCS_GTVSUVCENTER,C_GTVSUVCENTER = Extract_C_AUC(c,'HPVNEG_Double','SUVCENTER+GTV')\n",
    "ee,gg=(Extract_BEST_Score(C_GTVSUVCENTER,'HPVNEG_Double','SUVCENTER+GTV'))\n",
    "print('Best GTV+SUVCENTER SCORE',ee,'BEST C',gg)\n",
    "AUCS_GTVSUVMAX,C_MTVSUVMAX = Extract_C_AUC(c,'HPVNEG_Double','SUVCENTER+MTV')\n",
    "hh,ii=(Extract_BEST_Score(C_MTVSUVMAX,'HPVNEG_Double','SUVCENTER+MTV'))\n",
    "print('Best MTV+SUVMAX SCORE',hh,'BEST C',ii)'''\n",
    "\n",
    "# Du kan kjøre på nytt, og velge C-verdiene med høyest AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC CURVES\n",
    "The ROC curves are plotted for three patient groups. This is to get a view, and a visualization of how things look.\n",
    "- STD er sterkt påvirket av mengde data. Vi ser at den store pasient gruppen har mindre std sammenlignet med de andre. Nesten halvparten. De andre gruppe pasientene har like stor std. Kanskje nederland data vil forberede dette?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import auc\n",
    "#from scikitplot.metrics import plot_roc_curve\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Funcion that produces extrapolated tprs with a base fpr. Also it plots a number of ROV-curves.\n",
    "def ROC_tpr_fpr(C,Input,Predictor,size):\n",
    "    Ydata,Xdata = AllData(Input,Predictor)\n",
    "    X = np.array(Xdata) # Predictors stored here\n",
    "    y = np.array(Ydata)# Outcome\n",
    "    logreg = LogisticRegression(C = C ).fit(X, y)\n",
    "    y_score = logreg.predict_proba(X)\n",
    "    fpr, tpr, _ = roc_curve(y, y_score[:, 1])\n",
    "    AUC = auc(fpr, tpr)\n",
    "    print('AUC for whole dataset',AUC)\n",
    "    plt.plot(fpr, tpr, 'r', alpha=1)\n",
    "    base_fpr = np.linspace(0, 1, 101) # I will extrapolate this later tpr. \n",
    "    idx = np.arange(0, len(y))\n",
    "    # Will use this to later shuffle my data\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    for j in np.random.randint(0, high=10000, size=size): # Velger tilfeldig j mellom 0 og 10 000. Velger ut 10 tall.\n",
    "        np.random.shuffle(idx) # Jeg mixer tallene i idx\n",
    "        cv = StratifiedKFold(n_splits=5,random_state=j) # Cross validation. Split my data in 5\n",
    "        for i, (train, test) in enumerate(cv.split(X,y)):\n",
    "            logreg = LogisticRegression(C = C ).fit(X[idx][train], y[idx][train])\n",
    "            y_score = logreg.predict_proba(X[idx][test])\n",
    "            fpr, tpr, _ = roc_curve(y[idx][test], y_score[:, 1])\n",
    "            AUC = auc(fpr, tpr)\n",
    "            aucs.append(AUC)\n",
    "            plt.plot(fpr, tpr, 'b', alpha=0.05)\n",
    "            tpr = np.interp(base_fpr, fpr, tpr)\n",
    "            tpr[0] = 0.0\n",
    "            tprs.append(tpr)\n",
    "    return tprs,base_fpr,aucs\n",
    "\n",
    "# Insert Data information here. \n",
    "def ROC_PLOT_CURVE(C,Input,Predictor,size):\n",
    "    tprs,base_fpr,aucs = ROC_tpr_fpr(C,Input,Predictor,size)\n",
    "    tprs = np.array(tprs)\n",
    "    mean_tprs = tprs.mean(axis=0)\n",
    "    std = tprs.std(axis=0)\n",
    "\n",
    "    tprs_upper = np.minimum(mean_tprs + std, 1)\n",
    "    tprs_lower = mean_tprs - std\n",
    "    mean_auc = auc(base_fpr, mean_tprs)\n",
    "    std_auc = np.std(aucs)\n",
    "    print('Mean AUC is',mean_auc,'With a STD',std_auc)\n",
    "\n",
    "\n",
    "    plt.plot(base_fpr, mean_tprs, 'b')\n",
    "    plt.fill_between(base_fpr, tprs_lower, tprs_upper, color='grey', alpha=0.3)\n",
    "\n",
    "    plt.plot([0, 1], [0, 1],'g--')\n",
    "    plt.xlim([-0.01, 1.01])\n",
    "    plt.ylim([-0.01, 1.01])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "\n",
    "    plt.show()\n",
    "'''print('ROC CURVE FOR HPV NEG PATIENTS WITH MTV AS PREDICTOR')\n",
    "print(ROC_PLOT_CURVE(22.4970052592174,'NEG','MTV',10))\n",
    "\n",
    "print('ROC CURVE FOR HPV NEG PATIENTS WITH GTV AS PREDICTOR')\n",
    "print(ROC_PLOT_CURVE( 22.4970052592174,'NEG','GTV',10))\n",
    "\n",
    "print('ROC CURVE FOR HPV NEG PATIENTS WITH SUVMAX AS PREDICTOR')\n",
    "print(ROC_PLOT_CURVE(1,'NEG','SUVMAX',10))\n",
    "\n",
    "print('ROC CURVE FOR HPV NEG PATIENTS WITH SUVMAX and MTV AS PREDICTORS')\n",
    "print(ROC_PLOT_CURVE(0.0001,'HPVNEG_Double','SUVMAX+MTV',30))\n",
    "\n",
    "\n",
    "\n",
    "print('ROC CURVE FOR HPV POS PATIENTS WITH MTV AS PREDICTOR')\n",
    "print(ROC_PLOT_CURVE(2.3974349678010785,'POS','MTV',10))\n",
    "\n",
    "print('ROC CURVE FOR HPV POS PATIENTS WITH GTV AS PREDICTOR')\n",
    "print(ROC_PLOT_CURVE(44.21499907374505,'POS','GTV',10))\n",
    "\n",
    "print('ROC CURVE FOR HPV POS PATIENTS WITH SUVMAX AS PREDICTOR')\n",
    "print(ROC_PLOT_CURVE(1e-10,'POS','SUVMAX',10))\n",
    "\n",
    "print('ROC CURVE FOR HPV POS PATIENTS WITH SUVMAX and MTV AS PREDICTORS')\n",
    "print(ROC_PLOT_CURVE(0.4171124612056533,'HPVPOS_Double','SUVMAX+MTV',0))\n",
    "\n",
    "print('ROC CURVE ALL PATIENTS WITH MTV AS PREDICTOR')\n",
    "print(ROC_PLOT_CURVE(1e-10,'All','MTV',10))\n",
    "\n",
    "print('ROC CURVE FOR ALL PATIENTS WITH GTV AS PREDICTOR')\n",
    "print(ROC_PLOT_CURVE(1e-10,'All','GTV',20))\n",
    "\n",
    "print('ROC CURVE FOR ALL PATIENTS WITH SUVMAX AS PREDICTOR')\n",
    "print(ROC_PLOT_CURVE(1e-10,'All','SUVMAX',20))\n",
    "\n",
    "print('ROC CURVE FOR ALL PATIENTS WITH SUVMAX and MTV AS PREDICTORS')\n",
    "print(ROC_PLOT_CURVE(1e-10,'HPVPOS_Double','SUVMAX+MTV',20))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "# Accurancy on Training\n",
    "print('The training accurancy is', logreg.score(XTrain,yTrain))\n",
    "print('The test accurancy is', logreg.score(XTest,yTest))\n",
    "\n",
    "#Classification report\n",
    "print(classification_report(yTrain,y_pred))\n",
    "\n",
    "# Confusion matrix plot function\n",
    "\n",
    "# import required modules\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_confusion_matrix(cm,classes=None,title='Confusion matrix'):\n",
    "    if classes is not None:\n",
    "        sns.heatmap(cm,xticklabels = classes,yticklabels=classes,annot=True,annot_kws={'size':30})\n",
    "    else:\n",
    "        sns.heatmap(cm,vmin=0.,vmax=1.)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "cm = confusion_matrix(yTrain,y_pred)\n",
    "cm_norm = cm/cm.sum(axis=1).reshape(-1,1)\n",
    "plot_confusion_matrix(cm_norm,classes=logreg.classes_,title='Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking on how outcome VS feature looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that does analyze on the min_samples_split, and also maximum number of leaves\n",
    "# What we end up with is the R2-score and MSE as the parameters are changing each at a time!(not at the same time)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "\n",
    "def DecisionTreeAnalyzor(Input,Predictor,depth,parameter,DataType,MinSampleSplit,MaxNumOfLeafNodes):\n",
    "    global score\n",
    "    y,X = AllData(Input,Predictor)\n",
    "    AllScore = []\n",
    "    ScorieTestTrain = 0\n",
    "    MaxieDepth = 0\n",
    "    MiniSampleSplit = 0\n",
    "    for i in range(1,depth): # Run over different depth levels\n",
    "        if parameter == 'MinSampleSplit': # Specify here which parameter we want to the analyze on\n",
    "            Component = MinSampleSplit\n",
    "        else:\n",
    "            Component = MaxNumOfLeafNodes\n",
    "        for j in Component: # Loop of the parameter\n",
    "            if parameter == 'MinSampleSplit':\n",
    "                # Chose the parameters. Here max_depth is iterated over deoth, whereas min_samples_split is changing. See that maximum number of leaves is contant = 100\n",
    "                model = DecisionTreeClassifier(max_depth = i,min_samples_split = j,max_leaf_nodes = 100,min_weight_fraction_leaf=0.001,random_state=42)\n",
    "            else:\n",
    "                #Here the maximum number of leaves changes whereas minimum number of samples before split is constantly = 100\n",
    "                model = DecisionTreeClassifier(max_depth = i,min_samples_split = 100,max_leaf_nodes = j,min_weight_fraction_leaf=0.001,random_state=42)\n",
    "            #Fit of model\n",
    "            model.fit(X,y)\n",
    "            cv = KFold(n_splits=20, shuffle=True)\n",
    "            #use k-fold CV to evaluate model\n",
    "            score = cross_val_score(model, X, y, scoring='accuracy',cv=cv, n_jobs=-1)\n",
    "'''            y_pred_Test = model.predict(XTest)\n",
    "            y_pred_Train = model.predict(XTrain)\n",
    "            Score_Train = metrics.accuracy_score(yTrain, y_pred_Train) \n",
    "            Score_Test = metrics.accuracy_score(yTest, y_pred_Test)\n",
    "            scoresum = Score_Train+Score_Test'''\n",
    "'''            #Calculate R2-score with help of SKLEARN kfold cross validation\n",
    "            if DataType == 'Train':\n",
    "                score = metrics.accuracy_score(yTrain, y_pred_Train)\n",
    "            else:\n",
    "                score = metrics.accuracy_score(yTest, y_pred_Test)\n",
    "            if scoresum > ScorieTestTrain and Score_Test > 0.65:\n",
    "                ScorieTestTrain = scoresum\n",
    "                MaxieDepth = i\n",
    "                MiniSampleSplit = j'''\n",
    "            # Also here, we calculate the MSE. Differentiate between training or Test data\n",
    "            '''if parameter == 'MinSampleSplit':\n",
    "                print(\"Accurancy Score: {:.2f}\".format(score),\"Depth: {:.2f}\".format(i),\"MinSampleSplit: {:.2f}\".format(j))\n",
    "            else:\n",
    "                print(\"Accurancy Score: {:.2f}\".format(score),\"Depth: {:.2f}\".format(i),\"MaxNumOfLeafNodes: {:.2f}\".format(j))'''\n",
    "\n",
    "    #print('Highest Score is ;',HighestScore,'With a Depth',MaxieDepth, 'and MinSampleSplit',MiniSampleSplit)\n",
    "    return np.mean(score)\n",
    "# Input parameters\n",
    "MinSampleSplit = [2,5,7,10,20,30,50,80,100,200,400,500,600,700,800,1000,1600]\n",
    "MaxNumOfLeafNodes = [2,10,30,60,100,150,250]\n",
    "#Be sure to delete the \"extra\" t, OR add if you want to analyze minimum number of samples.\n",
    "parameter = ('MinSampleSplitt')\n",
    "DataType = ('Test') # Insert in datatype so that we are sure we are using train data. Or\n",
    "j = DecisionTreeAnalyzor(30,'MinSampleSplit','Test',MinSampleSplit,MaxNumOfLeafNodes)\n",
    "print(j)\n",
    "'''k = DecisionTreeAnalyzor(30,'MinSampleSplit','Train',MinSampleSplit,MaxNumOfLeafNodes)\n",
    "print('Highest Score Test',j)\n",
    "print('Highest Score Train',k)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Train/Test split\n",
    "y,X = AllData('NEG','ECOG+MTV')\n",
    "dec_tree = tree.DecisionTreeClassifier()\n",
    "pipe = Pipeline(steps=[('dec_tree', dec_tree)])\n",
    "criterion = ['gini', 'entropy']\n",
    "max_depth = [2,4,6,8,10,12]\n",
    "min_samples_split = [7,8,9,10,11,12,13,14,15,16,17,18,19,20,21]\n",
    "min_weight_fraction_leaf=[0]\n",
    "min_samples_leaf = [1,2,3,4,5,7,10,12,14,15]\n",
    "parameters = dict(    dec_tree__criterion=criterion,\n",
    "                      dec_tree__max_depth=max_depth,dec_tree__min_samples_split=min_samples_split,dec_tree__min_samples_leaf=min_samples_leaf,dec_tree__min_weight_fraction_leaf = min_weight_fraction_leaf)\n",
    "Kfold=KFold(n_splits=70, shuffle=True)\n",
    "clf_GS = GridSearchCV(pipe, parameters,scoring = 'accuracy',cv=Kfold)\n",
    "clf_GS.fit(X, y)\n",
    "print(clf_GS.best_score_)\n",
    "print('Best Criterion:', clf_GS.best_estimator_.get_params()['dec_tree__criterion'])\n",
    "print('Best max_depth:', clf_GS.best_estimator_.get_params()['dec_tree__max_depth'])\n",
    "print('Best min_samples_split:', clf_GS.best_estimator_.get_params()['dec_tree__min_samples_split'])\n",
    "print('Best min_weight_fraction_leaf:', clf_GS.best_estimator_.get_params()['dec_tree__min_weight_fraction_leaf'])\n",
    "print('Best min_samples_leaf:', clf_GS.best_estimator_.get_params()['dec_tree__min_samples_leaf'])\n",
    "print(); print(clf_GS.best_estimator_.get_params()['dec_tree'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.7714285714285715 for both MTV and MTV+SUVCENTER\n",
    "0.71 for Ecog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition, datasets\n",
    "from sklearn import tree\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Train/Test split\n",
    "y,X = AllData('HPVNEG_Double','SUVCENTER+MTV')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "dec_tree = RandomForestClassifier()\n",
    "pipe = Pipeline(steps=[('dec_tree', dec_tree)])\n",
    "#model = DecisionTreeClassifier(max_depth = i,min_samples_split = j,max_leaf_nodes = 100,min_weight_fraction_leaf=0.001,random_state=42)\n",
    "criterion = ['gini', 'entropy']\n",
    "max_depth = [2,4,6,8,10,12]\n",
    "min_samples_split = [7,8,9,10,11,12,13,14,15,16,17,18,19,20,21]\n",
    "min_weight_fraction_leaf=[0]\n",
    "min_samples_leaf = [1,2,3,4,5,7,10,12,14,15]\n",
    "parameters = dict(dec_tree__criterion=criterion,\n",
    "                      dec_tree__max_depth=max_depth,dec_tree__min_samples_split=min_samples_split,dec_tree__min_samples_leaf=min_samples_leaf,dec_tree__min_weight_fraction_leaf = min_weight_fraction_leaf)\n",
    "Kfold=KFold(n_splits=70, shuffle=True)\n",
    "clf_GS = GridSearchCV(pipe, parameters,scoring = 'accuracy',cv=Kfold)\n",
    "clf_GS.fit(X, y)\n",
    "print(clf_GS.best_score_)\n",
    "print('Best Criterion:', clf_GS.best_estimator_.get_params()['dec_tree__criterion'])\n",
    "print('Best max_depth:', clf_GS.best_estimator_.get_params()['dec_tree__max_depth'])\n",
    "print('Best min_samples_split:', clf_GS.best_estimator_.get_params()['dec_tree__min_samples_split'])\n",
    "print('Best min_weight_fraction_leaf:', clf_GS.best_estimator_.get_params()['dec_tree__min_weight_fraction_leaf'])\n",
    "print('Best min_samples_leaf:', clf_GS.best_estimator_.get_params()['dec_tree__min_samples_leaf'])\n",
    "print(); print(clf_GS.best_estimator_.get_params()['dec_tree'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import decomposition, datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Train/Test split\n",
    "y,X = AllData('HPVNEG_Double','SUVCENTER+MTV')\n",
    "\n",
    "\n",
    "dec_tree = GradientBoostingClassifier()\n",
    "pipe = Pipeline(steps=[('dec_tree', dec_tree)])\n",
    "#model = DecisionTreeClassifier(max_depth = i,min_samples_split = j,max_leaf_nodes = 100,min_weight_fraction_leaf=0.001,random_state=42)\n",
    "max_depth = [2,4,6,8,10,12]\n",
    "learning_rate=[0.1,0.2,0.3,0.5]\n",
    "min_samples_split = [7,8,9,10,11,12,13,14,15,16,17,18,19,20,21]\n",
    "min_samples_leaf = [1,2,3,4,5,7,10,12,14,15]\n",
    "#parameters = dict(dec_tree__max_depth=max_depth,dec_tree__learning_rate = learning_rate)\n",
    "parameters = dict(dec_tree__learning_rate = learning_rate,\n",
    "                      dec_tree__max_depth=max_depth,dec_tree__min_samples_split=min_samples_split,dec_tree__min_samples_leaf=min_samples_leaf,dec_tree__min_weight_fraction_leaf = min_weight_fraction_leaf)\n",
    "\n",
    "Kfold=KFold(n_splits=50, shuffle=True)\n",
    "clf_GS = GridSearchCV(pipe, parameters,scoring = 'accuracy',cv=Kfold)\n",
    "clf_GS.fit(X, y)\n",
    "print(clf_GS.best_score_)\n",
    "print('Best Criterion:', clf_GS.best_estimator_.get_params()['dec_tree__criterion'])\n",
    "print('Best max_depth:', clf_GS.best_estimator_.get_params()['dec_tree__max_depth'])\n",
    "print('Best min_samples_split:', clf_GS.best_estimator_.get_params()['dec_tree__min_samples_split'])\n",
    "print('Best min_weight_fraction_leaf:', clf_GS.best_estimator_.get_params()['dec_tree__min_weight_fraction_leaf'])\n",
    "print('Best min_samples_leaf:', clf_GS.best_estimator_.get_params()['dec_tree__min_samples_leaf'])\n",
    "print(); print(clf_GS.best_estimator_.get_params()['dec_tree'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(max_depth = 2,min_samples_split = 7,max_leaf_nodes = 100,min_weight_fraction_leaf=0,random_state=42)\n",
    "model.fit(X,y)\n",
    "print(model.feature_importances_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By changing we see that sensitive parameters seem to be the depth and MinSampleSplit. MaxNumOfLeafNodes does not contribute much. So, we shall drop the latter. Vi går for depth = 3, og minSampleSplitt = 80. Det virker mest riktig. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create Decision Tree classifer object\n",
    "clf = DecisionTreeClassifier(max_depth=4,min_samples_split=2)\n",
    "\n",
    "# Train Decision Tree Classifer\n",
    "clf = clf.fit(XTrain,yTrain)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(XTest)\n",
    "print(y_pred)\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy Decision Trees:\",metrics.accuracy_score(yTest, y_pred))\n",
    "\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#Create KNN Classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=2)\n",
    "\n",
    "#Train the model using the training sets\n",
    "knn.fit(XTrain, yTrain)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = knn.predict(XTest)\n",
    "print('This',y_pred)\n",
    "\n",
    "print(\"Accuracy K-nearest :\",metrics.accuracy_score(yTest, y_pred))\n",
    "\n",
    "\n",
    "# Training a SVM classifier using SVC class\n",
    "svm = SVC(kernel= 'linear', random_state=1, C=0.1)\n",
    "svm.fit(XTrain, yTrain)\n",
    " \n",
    "# Mode performance\n",
    " \n",
    "y_pred = svm.predict(XTest)\n",
    "print('Accuracy SVM: %.3f' % accuracy_score(yTest, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(4,2), solver='adam', max_iter=500)\n",
    "mlp.fit(XTrain,yTrain)\n",
    "\n",
    "predict_train = mlp.predict(XTrain)\n",
    "predict_test = mlp.predict(XTest)\n",
    "print(\"Accuracy NN :\",metrics.accuracy_score(yTest, predict_test))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "y,x = AllData('NEG','MTV')\n",
    "\n",
    "\n",
    "\n",
    "std_slc = StandardScaler()\n",
    "dec_tree = tree.DecisionTreeClassifier()\n",
    "n_components = list(range(1,2,1))\n",
    "criterion = ['gini', 'entropy']\n",
    "min_weight_fraction_leaf=[0,0.1,0.02,0.5]\n",
    "max_leaf_nodes = [2,5,8,15,25,35]\n",
    "pipe = Pipeline(steps=[('std_slc', std_slc),('dec_tree', dec_tree)])\n",
    "print(pipe)\n",
    "max_depth = [2,4,6,8]\n",
    "min_samples_split = [7,8,9,10,11,12,13,14,15,16,17,18,19,20,21]\n",
    "min_weight_fraction_leaf=[0]\n",
    "min_samples_leaf = [1,2,3,4,5,7,10,12,14,15]\n",
    "Kfold=KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "parameters = dict(pca__n_components=n_components,dec_tree__criterion=criterion,dec_tree__max_depth=max_depth,\n",
    "                  dec_tree__min_samples_split=min_samples_split,\n",
    "                  dec_tree__min_samples_leaf=min_samples_leaf,dec_tree__max_leaf_nodes=max_leaf_nodes)\n",
    "clf = GridSearchCV(pipe, parameters, n_jobs=4,cv=Kfold)\n",
    "clf.fit(X=x, y=y)\n",
    "tree_model = clf.best_estimator_\n",
    "print (clf.best_score_, clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "y,x = AllData('NEG','MTV')\n",
    "\n",
    "dec_tree = RandomForestClassifier()\n",
    "pipe = Pipeline(steps=[('std_slc', std_slc),('dec_tree', dec_tree)])\n",
    "parameters = dict(dec_tree__criterion=criterion,dec_tree__max_depth=max_depth,\n",
    "                  dec_tree__min_samples_split=min_samples_split,dec_tree__min_samples_leaf=min_samples_leaf)\n",
    "Kfold=KFold(n_splits=20, shuffle=True)\n",
    "clf = GridSearchCV(pipe, parameters, n_jobs=4,cv = Kfold)\n",
    "clf.fit(X=x, y=y)\n",
    "tree_model = clf.best_estimator_\n",
    "print (clf.best_score_, clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important Variables are stored here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store PET_SUV_ALL\n",
    "%store SUV_ALL\n",
    "%store SUV_HPVPOS\n",
    "%store SUV_HPVNEG\n",
    "%store Accuracy\n",
    "%store F1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%history -g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
